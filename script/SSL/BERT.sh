torchrun \
    --standalone \
    --nproc_per_node=2 \
    run.py \
    --task_name ssl \
    --model BERT \
    --learning_rate 2e-3 \
    --normalize \
    --patience 0 \
    --num_epochs 150 \
    --e_layers 2 \
    --d_model 256 \
    --mask_ratio 0.5 \
    --activation "gelu" \
    --train_batch_size 128 \
    --test_batch_size 128 \
    --dropout 0.3 \
    --linear_dropout 0.5 \
    --num_workers 12 \
    --use_scheduler \
    --warmup_epochs 20 