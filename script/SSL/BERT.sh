torchrun \
    --standalone \
    --nproc_per_node=4 \
    run.py \
    --task_name ssl \
    --model BERT \
    --learning_rate 2e-3 \
    --normalize \
    --patience 0 \
    --num_epochs 150 \
    --e_layers 2 \
    --d_model 256 \
    --mask_ratio 0.3 \
    --activation "gelu" \
    --train_batch_size 128 \
    --test_batch_size 128 \
    --dropout 0.3 \
    --linear_dropout 0.5 \
    --dataset TUAB \
    --root_path "/data/guihaokun/resample/tuh_eeg_abnormal/" \
    --log_dir "/home/guihaokun/Time-Series-Pretrain/logging_ab" \
    --num_workers 10 \
    --use_scheduler \
    --warmup_epochs 20 